{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zoldyck13/Qwen-7B-Chat-Fine-Tuning/blob/main/Qwen_7B_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A simple colab to fine-tune Qwen Chat models.\n",
    "\n"
   ],
   "metadata": {
    "id": "WXZ7SxQUVuth"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup(run once per session)"
   ],
   "metadata": {
    "id": "YZnheG7CXfEi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code mounts your google drive, for save the fine-tuned weights."
   ],
   "metadata": {
    "id": "1SZFCmxTXvCp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "v-Wg1vhoYGg3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "607f79ad-a8c7-4c49-bac7-3bedb9993fa4"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Download the model from the huggingface_hub."
   ],
   "metadata": {
    "id": "ll_aydrGYSUh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"Qwen/Qwen-7B-Chat\",local_dir=\"./models/Qwen-7B-Chat\", local_dir_use_symlinks=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b34dc78a8be2468d97da3d62c3585d87",
      "5b40df3af8344d99b28c4431e87c6990",
      "f5f21c6aa68e4e069e29ab6d887128d0",
      "82e7dc9a143044749fcabcb67678a3aa",
      "5517a2d6876047c3a00f650655ebdbdc",
      "6227145247f34cefba5f2b167135f3af",
      "71fd7358f2254073a246cad3a360f517",
      "5bef1bee533b40ea83c7024b4860a57e",
      "b6f0fbb9cd5c47039ed77daa2aefd339",
      "7fc94fe5f30f48408922e399c527845d",
      "6943fe5eb106476fb3887208bd2f7388",
      "8707cc718acf4f19bf2c2d55a215acef",
      "0b1fdff20cfc43508be22e56be378a35",
      "e89fa00acfd74878b1ffea54fb88c3fb",
      "ef8966b0b4654420aa008548dab9fbd1",
      "a16b2370132b4a58bef0e62683abb236",
      "0abcab3239054bcc8e1831a6927bda0d",
      "ecd447e0171b416994a49484671ad553",
      "50b196555aea42089c0b957d2b31ebf2",
      "6a2dd9bfd6dc4d21909ec50fbbbd450e",
      "378d092e1c774a469fd475476e3ae32f",
      "0b1e536cd1f8417f8b751ca61be9fcba",
      "bb0d543551184369acdfd2c822858fe2",
      "bcb9291e2cf24b94ae619c16d67783e1",
      "41d896017a3542e6beeda6b7456738f4",
      "6d1d7dc35cc24a3490da4d183b161053",
      "b06908c5320a4ae386bc914d8457f828",
      "e174e2cd91da48abb6df28dbcb1fc0c4",
      "7aecdbd539cd466389a6ae8eb2a274c6",
      "e9a00a19b705451094280f2641d57068",
      "3ceaa45b04fc4783b94a89770c8983ea",
      "fcbffa7e642f4c48b90f7c0618c774dc",
      "d2b759c0ceb743b28d7db1da7722c480",
      "eedad553c5b747fb814010e62b2f5dc5",
      "b88edf87d52b447a9246125fe2423d29",
      "df631f5fd9be43dabb02a1e9e494c56a",
      "53909dfc10314bb49cc5476f935d394c",
      "26a8912eef964d869b81f578c2bb3fa1",
      "25aa131ddd28485ab3c230791ec086db",
      "3025e71a66b8486eb82db8188faf572d",
      "3ec86004f0bc408c93bed7f43f980d74",
      "1b042ee38fcc4af493f97d9f4d0290d9",
      "bf3d1ba095ec49ddaf0529dcd04b6152",
      "9a164d49c9854472b02524b86fc433f7",
      "7173f734bc434889a5c5c97536215a07",
      "f3e74ea0699149fdab40ffb18e0fbaeb",
      "20302ff9bb2f4c1fbe43b02e7dba7687",
      "7eae3432272143e9a45db688e0200f73",
      "ceed565b066f4492a11194cd32676c9d",
      "77eaa2d6cad34196837b5ea6b48a08ac",
      "b5b269202cfa47919d7e3cb4918c9d81",
      "d4db1bc5550a4710b54685b4bc054e05",
      "2e659897b32c41aa9b80793d7611e204",
      "e4a4b13ecea745b2b488efd64d5a359b",
      "d47e50bc331245faa3ce16cdb1a9f3e3",
      "8adc65644eb94515a97a393aa04b8b7d",
      "f4532bc36f954839b41c60c62e29004b",
      "8c637fbdf8b4416eb82722f41e0c3f8b",
      "19c99e96e66f4fdf9df32584f45ec4f8",
      "844fb3c7f4c54e0c89d402bc8da9415a",
      "4dad9a8f7bcb4ead81e14adc501c613f",
      "a2086c92450a4744b1dae78169f003bd",
      "7c48b3fd360f4b0bacf719ac43147efc",
      "4e1a0825f9a142d798cad4c3f56924ca",
      "080c637cb14842648654a7ec710ec904",
      "317e41d6f6324c9eae5e0066e88aa7f3",
      "018cdc77ada84c75965c25a8ff00fee6",
      "fbf9ddcab3464a6c84507b0037d2fa0b",
      "232f727037af414b8b013ad15288259e",
      "3013e8d75f6343c0a74c3873c1c47c72",
      "790c99d2108d4a84bad3fd0144034292",
      "a9d821873c8a474c8af56f360418dcf4",
      "916a41f95459467ba214dd84da1e4713",
      "2d60be3a6fd84e648b4c34caa80aad3d",
      "bd5b70afdc4548b09ce1cbc2140ae026",
      "0c42acbbece04330a9e215bd8d5da9c2",
      "0689799cde9f49a1bb57ccde3359f95b",
      "cdc5c9d659834603932453edf107b385",
      "ba0f218eb5344f0aa4b3fb9f77e374c0",
      "fc1f7840fe88480392cb1d307f644278",
      "d25e5bcf083e4b0897048c742c082020",
      "76e86c25cdd04ed0853b2f3efdf4e7ee",
      "1f4f6d91118f4994bb43365bd6827131",
      "d6f01010003f4cb39e75704f276d8637",
      "ba74a27d6c594cb1801c7a26a7a1e2d3",
      "c134a46234434d28b7049968c77669d2",
      "862e7846fd5a41ab9d93394c48036cb1",
      "6b3e40e2a7e74cbab129c8cebad43eb7",
      "7f5cfa4d9a6e439992102ad9667e4e7b",
      "489f812f021343b4b827214660999a36",
      "567affcb057641f697d1b64f08588aa8",
      "7ead93c4aeb249a5a332d5e04d45dffa",
      "c163e2315197495eb68b90538ba59bff",
      "288ffab013c54f4d89f131c5c61024ee",
      "98b116990b0c48cf8aab30e4f4dfb059",
      "f3d6c02ef1a94aa7ab3625fafa55a46d",
      "af424585aa59422c8b46a3c0a0c08b72",
      "3446d3c998d1443cbbe8593e50902b84",
      "c1003420da324da1bdbb72655706564f",
      "70977dadd43d4323850ac11f1f6c7334",
      "a942e80e3ff743a28f4a33873d85a981",
      "c9f6f60ce40d425c9aa366d801d72a01",
      "50dfb6c453f045a9bcb7402de2ee4dea",
      "1aa0bef7aa1940dc96f5d71ab68ac27e",
      "c0c2548f478f4475a7e8befe6bd33813",
      "129bcb5fa95349f6bd05652bac85f4d6",
      "584dc9ac726548ceb6cdb23f6c83d40a",
      "ef73429341fa44c9bb2c582aae98e995",
      "9a52d01ff7ef4c4795e6c95fd2a767dc",
      "bfc01c3b8eda473e87e99d391f6e66da",
      "5e648c8305cb47c0b9c2f55a375a4e76",
      "e487ebcb93254d5faddcf9bd9e771a7b",
      "ae761160d9b34e0f8357e3fcdf471740",
      "f1e49bd3f77d4856bdb7a0c17d7bf1a0",
      "fb64c3a7f8a346718703aa857c6049e4",
      "6e3a523dcf804dcc8941dd3c9b824c11",
      "62d5929524e84c809364176b2f3fa086",
      "2013d698a64c4b3aa39dd61e3b28fb03",
      "7b7a55d8c12a40ada6c2c6eecd861954",
      "3df2f7e16135476882ac2be326f9cb47",
      "76129cae007248acbc33785edde4bc52",
      "06fe21bf54c6481bad35fc50ca8644ab",
      "ba1f061538654280a540f843f4565e49",
      "e8d90835b8d4412c9674a88c009cba4a",
      "f661ae36ee0c4a51b7333e24ae6fe4a6",
      "0159a37ae2c14ab7833bb27c8d24af03",
      "a79fbfaff46f436b88c467106c8d4702",
      "af878f3e227943888313a9d0e72f9f2c",
      "764bee774a48498ea40dcf9711f82fce",
      "a22a666de5264ea9b53b2cc75b9b31e2",
      "15de7b64d3db4c7dbc9a97917043263d",
      "e55bac280c1447849842b7a4dcb6a8dd",
      "75df964837944bb3aebab4288dbf7d2a",
      "afd784e480ff40cc9fce63cac4246987",
      "09d17e562d344a6a93c82ad87b4c5723",
      "1a3c3d1a3903453b97d169032b62a840",
      "b1ba86f754c94819992bf2940cec6499",
      "73e7259fafd6410886bda4886495969d",
      "af82560e7ec646cf99905e9002c216a1",
      "98f31d8375bb4b9994ab814b839268af",
      "4761449d95934508a3a5a6410c4d54c7",
      "8a28a9c8320642c684818c6aad21741c",
      "0fc9b58f2c9b48c899118bac9afe1f39",
      "9b7aa80bd2a24c1a8c38cbfb78413545",
      "4f8b49f620c5470d846ed7cd616f23cd",
      "e3f51a84768b43e59ffeefd2e1fc2de6",
      "e755f2bf6c8a4e898467d715ce46740b",
      "558627d3f4fd4b089660bcddfeac5550",
      "aa60a504fb1848bd947a79568137dc25",
      "afc5e98097274642afcfe102c54339bf",
      "fd4fba7dd56d415c9d31038253274608",
      "209a877a237b495f9b800baf22ac146f",
      "35db04e631854116ba808c28fb32c5c1",
      "e5db7fc6984c4fdba66122155463a4e9",
      "6499d3e23e044f2d9a4979e856d33f86",
      "003a086944084dc88136153072b64276",
      "28a2e7d365d945fb929708901005b9a2",
      "acee768d0d4241e4bb3aece55ebe6e2b",
      "c2520027dd3b415d8c22930e742c4ff0",
      "77aee9480448418c8f85d986a6f0f235",
      "3698d225c9e943f882052b3eb328f4fa",
      "1391bf8cfd3648f7a11dc92e6b6fe8b2",
      "6e50c65839ea4ff7a25a9025a33f81bb",
      "d7e632357f5b4778ba36ce3976453601",
      "e1c6a935e6e04f369c3506962cbe9c32",
      "2d0c780dadc04ae2bb0a8178f0a637dd",
      "f9a066f2a88b409db8390cbc92a65bdc",
      "b26b006d8fe74ec3aa09b635919f0738",
      "d1b521b2047b476ba1595a9614e98b5e",
      "fde6f081e6e748e793e7bf9be06463b8",
      "55895427aa3f40feb1dfcabcd1f85f98",
      "5c0c3b61a59c45baa3411f3a9aaf4da5",
      "1fa5dd85c4634d1ab124af23afa5142a",
      "66d68e4ed82e4613b6b639ff711d5504",
      "84a5e5807f14463898fadc98eab4c360",
      "c4a73edb14a94447809adb21335e1b19",
      "f1410c5ce1c941e5a86beee59aa7e88b",
      "7fbea2d595d74570a4b9f492248d7f5b",
      "b0ff49d04d0a4a1183e08cc1c34182de",
      "3de7ecdf0cbc45c2b0ee4ae5fb4c37b9",
      "5d137bfe8b9d4d358535c685695901f6",
      "4917929634ee4957985c8626bdaeb0b7",
      "125ea761be084a80bc2766f7061fcc05",
      "53730f5984f24fcd88570e7a34dfc5c0",
      "ee1b98d215bf43f5ae4a12942c951aba",
      "b283c0bbc465439aa53384112767edea",
      "5d20e0d852274a848bb79c1586aa2d39",
      "fd50d866a9364a1b8f442aef874a30eb",
      "c54fff6028b14ce2b011424a6f93bcce",
      "56af469e54644d28b030f4f2d52c8290",
      "8c06797c47884a2282baf61238c3c8a0",
      "f0821d727c794a079b4c067e6c4dbe6a",
      "c7a2da2ec8514502b7e349d50f98e5e2",
      "934b8afb1a4c42798f4d3bd18793fbe4",
      "f3c3a965b70747df94f26b4b5d421883",
      "bc30fe2da9e044a7965f34fca49b9e0b",
      "dfc5cfc7bfc44bca851171a38b514437",
      "7ec8bdc6bcc44fa6a1d917841cd1bed8",
      "a927b111539d4b0b92436532a494b736",
      "5f65db70676b4350800359aea16df608",
      "012020faf63b4d668494022d481518da",
      "cd99d7b2bfff4208a8887cf307777620",
      "4aea9cb23faa43939b42274af7a2f945",
      "2bbf4d46c66c42049a786acdd1528526",
      "00cf4db736a44f8092a2f75ab1971495",
      "8581166b22bc4050ad8eb14f4855cb8f",
      "686a78a59ccb472e9f723f8c521f803f",
      "374edf7d1bc34d49801b02dee145c3cb",
      "c5699b440193409db10e6188cbee5c9a",
      "7b0b89560be74473a744e5b5e12435ce",
      "90f10a9d857643009c26bae0a5b6f2ed",
      "630b0a3568824ad38826b5a8dac5f447",
      "4a3cc560d6e24260962f78984d8becff",
      "68fc15e945c24e7fb900a50f508b994a",
      "4556607301e244ecb4e422e94bcca2c9",
      "8fa73ad284c04139b14ab19f1e82c08b",
      "88966d2e6d5544d1a25100f4a87cfdf8",
      "ab23476ded094692b4c6b01fe8a34bb8",
      "2be27a10fbb64099a3f8957ddf90f8a8",
      "35fbb0d794514152acd07d74e1364236",
      "51e6ad0ae65d4366b1749d7d56013b34",
      "65a3a3912b8f46fa88147db35bc53fd7",
      "74e4ba0dd6e441d592f8978abd21b4ef",
      "c505f7f6018e4ba2b798914b519c087c",
      "2e54072a539040cba906b37c01ecedfe",
      "d4f4f5694cc84dd795ea03f6c0f775d9",
      "264ca86ed8ad423bac9c201b6e4376b9",
      "28c01a6861714e6589d6ad814e3bac96",
      "df1dc6a592bf46b9bbad24303b7837ee",
      "1448c217aa2e455b94aeab99c3d0b89c",
      "f8686c89473a4b95bf830ae6f22b6302",
      "8688fb84113844f5b01d0fb6624e48d2",
      "0275d83140fb4a2ab944667d26ef4f98",
      "16be972b5de4473e87f3648e7d010214",
      "d92589e2668843158f779ee3f4edeb08",
      "f23c4a4f68f045ac89c5ddb5a92e723b",
      "66076f11ebdf4acc9483b65efb08fccd",
      "7a64da433c01426ea86d831b7c195bed",
      "40b07105f77f494dbdc0e13c1c6a14cb",
      "92676b9a195d4bb5bbf130846056dbac",
      "c745353544424c798fa2f66bbd953513",
      "a161e98823a146ab89bc64500082d794",
      "450e9501c97541b681a7826e6a439a41",
      "98e23ae3c937410a90c979dbc48fd61a",
      "47966ce396b84d4a9af80985d7f925ba",
      "be275dbc7acd433aa05b450857534a08",
      "946e9bd35f7a4706b31cadfca5242931",
      "f19a098d534b4b6fa46476bab9dbfcff",
      "4cbc2a1033a04c819e0950a02d602f67",
      "deb65d5ede114ba894794c58753ef556",
      "a52090cc2e6444d495ef900ffdef3954",
      "edbbc635de66423189793ac1b7d99ba5",
      "233a3361fda54f97af239a684c1c9a4c",
      "76edb83e422f4ee38e5dc6c8bceec64d",
      "c9b5a01d9974493583906777b9f71ce8",
      "6c9bbb5ffad943a1ae0a4aae3b0900bd",
      "51b36e62e6cd47149af4d2621b665c84",
      "397c0df56ce54b52b6e169baf3889d18",
      "3f24a87be16242b1b91f836eb0e7f830",
      "23271dd6c3f747dfbc464b479899553f",
      "9b282b0d11294f1ca68400e6f65e6de6",
      "3e164a86330d4f28b2028a7932cfb107",
      "b37b2a25107644238b111a52497ae453",
      "e67a577afccc412a912c3dd36901d22f",
      "f3707385cb4c40eab359094d6499603e",
      "2a05ed50fc544595804a91dcf34051b7",
      "2c34fbbcded34229abe668fd431740f1",
      "89056aa4659a4386970abf4cf2b34ac4",
      "c9cc888b44ba40debd2341c3ace43962",
      "9a2d2f846daf45c18dfa1c73107a974b",
      "9cc101e985a04b529549ce01345d56f3",
      "8a66db93fe374b2b890a4d918192c263",
      "30a12a2a0b06412f979502062a7dbda0",
      "445b53cc93aa42aca586e73bf79f0179",
      "6055cd71c4ed40ab9dd7aa42d70f816e",
      "24c113e906424e20aeee1b7917538d97",
      "fe80bce0033d47088419b8091d7186b9",
      "e6fb8de3377b4fda912c43d8d490a59e",
      "aa7b4da5771342f8abe7216dbc10d749",
      "682b2d2773f8409e96162b60c95e0817",
      "e35304e4b59f491a83a4c0505d8e2d95",
      "e394b8a080cf4fad8edf60545e7132f8",
      "c12e8bcbfe134301b4bee5a2eddf90bb",
      "183b0f7d778a47ebbc30285d025bef55",
      "f234062aadc04f0fb401ae314bde40fa",
      "22bf2f93ca774687a60c5f4ae08817da",
      "ae60b1d39d8c4976885ca2dd8aa325c6",
      "dfd372ce5c694086a6637bb11aa3c7f1",
      "17ce2ee95a8e4ea98d223dc48a5e19a3",
      "a3076f9c4f8848f88dfc7464dd3c3163",
      "aae722e5e60a45c486b3305cf362b16c",
      "f7ce13057c344848a6eef267cee4a917",
      "7a9e82ec29374a95ad782e21d6d1648d",
      "e1de314e664e44f1b753c188a07f4ddc",
      "59b1335f8add49549c0c57e4288c6fda",
      "254d57e24a88412bbcae4b5f9924f962",
      "6cd9db87d17046e5aad028e25fbfb8b3",
      "159f188b2d2a437097c1b66f7e8ca9bc",
      "49cd117666d741968064b52e033db4f8",
      "e0398ca4f92a41f28143048b2b947843",
      "4550c5d28d5442b2b79f63f5ca576bfc",
      "e828e8b51e0d4427be0f0b24a89846e0",
      "eb4c3fc774af41d2ac5342186f76ac5b",
      "1a812083659149ac8d439d53ed670194",
      "4832e3b4da9241debed1d973effbedca",
      "244812714a864a39a42627a57911f989",
      "ec90404785604904a07c1eb5918a2eca",
      "afc5835d87f74a2f826a1cae395555a2",
      "0243d93fef4f4732bd037e766290f564",
      "1aa63cbb16144c7d99b100f4da03b234",
      "5c61bf1eee924eaf8d39a43f269be4fa",
      "789a37f1eb1d4ce4986476b7064c9bd8",
      "ebf5ef9caf554b4b9bd09952960c5689",
      "8bd99f02779d4baeb2931744dd3b0714",
      "90e97398e73f4c97b000ab0fc7c5d92d",
      "5205e684d0574a719fd4f2f0739c26cb",
      "72d3bd66e68348c9a9d686500d41f229",
      "eba59a131eb244fe982b882ce61758f4",
      "5ada1817c6044d6cb532f9cfbdbd861d",
      "e2e6b22d52fd4742a2a95205244023b9",
      "e02367c4117b477ab6910fe6503981b9",
      "4c855b124a9743a28503815b3867118a",
      "75232b3269b14f11b7019a9257039303",
      "a7360336cf9d47e19dc31195cddb2f73",
      "de42265cfc304f70894e6387eaa4369b",
      "e7832aee01d749a6a937d7b66f942374",
      "430ba273c9a54d81839c913039752185",
      "6720c7e8a93641a59c574c98d695e618",
      "c7d40c5877ab43909f3733f4ef48b4bc",
      "5835d58a328b45eca40177086e9044ac",
      "bc56795e16eb4c339523c240b6d166ce",
      "f32fcc4a59b34389afd671f491a9c078",
      "5a2c9d0b2c85416099130e9eec4f421b",
      "480f0df602034d44a1dbcaa13c801b04",
      "056de1f0c4434ffe9767a4a812f4bd6d",
      "8662551b264e47438889fc2293900f15",
      "801758e884384154955c27430f45b5d0",
      "378708fae1fe48e08c2f959986106c65",
      "85fe6add23ff480eb431473f7e430dd8",
      "ed4736a131104fa9972c60c12f8ad223",
      "9783efe35c2c44a785c94b25bd92eb1d"
     ]
    },
    "id": "p0wjVo1ru2vG",
    "outputId": "1a9a189a-2219-43bb-fd02-c73eb2f13049"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b34dc78a8be2468d97da3d62c3585d87"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "NOTICE: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8707cc718acf4f19bf2c2d55a215acef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb0d543551184369acdfd2c822858fe2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eedad553c5b747fb814010e62b2f5dc5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "code_interpreter_showcase_001.jpg:   0%|          | 0.00/138k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7173f734bc434889a5c5c97536215a07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "react_showcase_001.png:   0%|          | 0.00/309k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8adc65644eb94515a97a393aa04b8b7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "logo.jpg:   0%|          | 0.00/82.7k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "018cdc77ada84c75965c25a8ff00fee6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdc5c9d659834603932453edf107b385"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "react_showcase_002.png:   0%|          | 0.00/630k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f5cfa4d9a6e439992102ad9667e4e7b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "wechat.png:   0%|          | 0.00/68.4k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70977dadd43d4323850ac11f1f6c7334"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/911 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e648c8305cb47c0b9c2f55a375a4e76"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "configuration_qwen.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06fe21bf54c6481bad35fc50ca8644ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/273 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75df964837944bb3aebab4288dbf7d2a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "cache_autogptq_cuda_256.cpp: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b7aa80bd2a24c1a8c38cbfb78413545"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "react_prompt.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6499d3e23e044f2d9a4979e856d33f86"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "cache_autogptq_cuda_kernel_256.cu: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d0c780dadc04ae2bb0a8178f0a637dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1410c5ce1c941e5a86beee59aa7e88b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "cpp_kernels.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd50d866a9364a1b8f442aef874a30eb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a927b111539d4b0b92436532a494b736"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b0b89560be74473a744e5b5e12435ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51e6ad0ae65d4366b1749d7d56013b34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8688fb84113844f5b01d0fb6624e48d2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "450e9501c97541b681a7826e6a439a41"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76edb83e422f4ee38e5dc6c8bceec64d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3707385cb4c40eab359094d6499603e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24c113e906424e20aeee1b7917538d97"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modeling_qwen.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae60b1d39d8c4976885ca2dd8aa325c6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "qwen.tiktoken: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "159f188b2d2a437097c1b66f7e8ca9bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "qwen_generation_utils.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0243d93fef4f4732bd037e766290f564"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenization_qwen.py: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2e6b22d52fd4742a2a95205244023b9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc56795e16eb4c339523c240b6d166ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/models/Qwen-7B-Chat'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisite"
   ],
   "metadata": {
    "id": "fg3yUpW5Ycyz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "accelerator is installed to help the Transformers library manage devices and improve training preformance automatically."
   ],
   "metadata": {
    "id": "uljuAPd7Ytvo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U accelerator\n",
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVvEpcMWKMW6",
    "outputId": "fc51ea31-4fac-4c05-8a5d-bbd9cf3a3f93"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting accelerator\n",
      "  Downloading accelerator-2024.9.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting setproctitle>=1.1.8 (from accelerator)\n",
      "  Downloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting bottle<0.13,>=0.12.7 (from accelerator)\n",
      "  Downloading bottle-0.12.25-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting waitress>=1.0 (from accelerator)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading accelerator-2024.9.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bottle-0.12.25-py3-none-any.whl (90 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.2/90.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bottle, waitress, setproctitle, accelerator\n",
      "Successfully installed accelerator-2024.9.13 bottle-0.12.25 setproctitle-1.3.6 waitress-3.0.2\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "transformers is used to load, customize, and train large language model like Qwen-7B easily usign a high-level API"
   ],
   "metadata": {
    "id": "lqvI5KMRZouo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install -U transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0AWyrh2U--6",
    "outputId": "e83e30fe-05c0-4b1c-c2a0-e1e886d4ae4c"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: transformers 4.53.2\n",
      "Uninstalling transformers-4.53.2:\n",
      "  Successfully uninstalled transformers-4.53.2\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting transformers\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.53.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###This code performs fine-tuning on the Qwen-7B-Chat language model using the ğŸ¤— Transformers library with the following key techniques:\n",
    " - LoRA (Low-Rank Adaptation) is applied using the peft library to reduce the\n",
    "number of trainable parameters and make training more efficient.\n",
    " - The model is loaded in 4-bit using BitsAndBytesConfig to save memory and allow training on a single GPU.\n",
    " - The dataset consists of multi-turn conversations, formatted as chat between roles like [user] and [assistant].\n",
    " - A custom data collator is used to handle padding and attention masks manually.\n",
    " - The training is done using the Trainer class from Hugging Face, with gradient accumulation and mixed precision (fp16) for better performance.\n",
    "\n",
    "This setup allows efficient fine-tuning of a large model like Qwen-7B even on limited hardware."
   ],
   "metadata": {
    "id": "RsCCAwcNaKzq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "model_name = \"./models/Qwen-7B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/Qwen-7B-Chat\", trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": \"cuda:0\"}\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": \"./finetune_data.jsonl\"}, cache_dir=None) # Removed this line\n",
    "# Load the JSONL file using pandas and convert to datasets Dataset\n",
    "df = pd.read_json(\"./finetune_data.jsonl\", lines=True)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    all_texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        chat = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            chat += f\"[{role}]: {content}\\n\"\n",
    "        all_texts.append(chat)\n",
    "    return tokenizer(all_texts, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def custom_data_collator(features):\n",
    "    input_ids = [torch.tensor(f[\"input_ids\"]) for f in features]\n",
    "    attention_mask = [torch.ones_like(ids) for ids in input_ids]\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask_padded = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_mask_padded,\n",
    "        \"labels\": input_ids_padded.clone(),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-lora-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=custom_data_collator,\n",
    "    train_dataset=tokenized_dataset, # Modified this line\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554,
     "referenced_widgets": [
      "4e763f876a784b549d7b08f967292041",
      "afd97998956f4b729817faea53b7c5e8",
      "6df170f429af4dcb8fee2b70c75e26ab",
      "47acaaf98d054c2191f78a6485a2871d",
      "dfc20020d62544f0a13572d3f62c4d9e",
      "90a1b6aa6eeb4533bf75bebd15a9b598",
      "6ad999a8ee7c4e95a302b8f2ea0473da",
      "fab1b0ba1d2c4ec499206ef31f533f60",
      "136541cf815e4a91a438c313840cd603",
      "677d729685b64cf5965905c2e3181cce",
      "d534e5803c834732853e93e718a9aaea",
      "14bd875509c9419cbe4373ace1210bfb",
      "e03269cf1dde400b9f986964676113dd",
      "1b3ac24e805742f8b6e8ccf7b49b04d5",
      "7f8a5732005d4741b7692edf4c41d63e",
      "9f68fa6d86144608afa03216fd89c3b0",
      "6660606b37954e04bb8373193a674909",
      "2bbc8247274147f7adaac1258566184f",
      "768bcfc2dee8494289c214edc7b471de",
      "16893205fb6d4e0190068e021d9eb861",
      "afdc1da5f7974499a355797991fbf0b0",
      "ec38fc33f70f4202bd11863cd5bfa343"
     ]
    },
    "id": "Em2wQM3SHE6t",
    "outputId": "6c619bfb-ce04-4bec-d615-a6e4d96966ab"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Try importing flash-attention for faster inference...\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e763f876a784b549d7b08f967292041"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/301 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14bd875509c9419cbe4373ace1210bfb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-2-4187507103.py:92: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malifage14\u001b[0m (\u001b[33malifage14-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250721_184202-ti3muc3s</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alifage14-none/huggingface/runs/ti3muc3s' target=\"_blank\">./qwen-lora-finetuned</a></strong> to <a href='https://wandb.ai/alifage14-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/alifage14-none/huggingface' target=\"_blank\">https://wandb.ai/alifage14-none/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/alifage14-none/huggingface/runs/ti3muc3s' target=\"_blank\">https://wandb.ai/alifage14-none/huggingface/runs/ti3muc3s</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:457: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 10/114 00:48 < 10:26, 0.17 it/s, Epoch 0.24/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If you're facing troubleshooting with loading the model in 4-bit quantization, try the following:"
   ],
   "metadata": {
    "id": "Z1lRLwL0bC8N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3MhAUg4SwNm",
    "outputId": "ff0fb50a-040e-4f9e-d5f5-af9174efd045"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing installation: bitsandbytes 0.42.0\n",
      "Uninstalling bitsandbytes-0.42.0:\n",
      "  Successfully uninstalled bitsandbytes-0.42.0\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Using cached bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "axolotl 0.12.0.dev0 requires bitsandbytes==0.46.0, but you have bitsandbytes 0.46.1 which is incompatible.\n",
      "axolotl 0.12.0.dev0 requires datasets==4.0.0, but you have datasets 2.18.0 which is incompatible.\n",
      "axolotl 0.12.0.dev0 requires peft==0.16.0, but you have peft 0.10.0 which is incompatible.\n",
      "axolotl 0.12.0.dev0 requires tokenizers>=0.21.1, but you have tokenizers 0.19.1 which is incompatible.\n",
      "axolotl 0.12.0.dev0 requires transformers==4.53.2, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.46.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the model with the fine-tuned weight from ./qwen-lora-finetuned/chechpoint to see the improvements in output quality."
   ],
   "metadata": {
    "id": "38nnwb74bv5H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import FlaxAutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"./models/Qwen-7B-Chat\"\n",
    "loar_weight_dir = \"./qwen-lora-finetuned/checkpoint-114\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = PeftModel.from_pretrained(model, loar_weight_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "input_text = \"Ø´Ù„ÙˆÙ†Ùƒ Ø­Ø¨ÙŠØ¨ÙŠ\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160,
     "referenced_widgets": [
      "107c13863fd14396bdffad51bbf0c207",
      "4a8488d5f66f4dd481736ff046feecfe",
      "4cd4077581c44a0a9b1420a5d9096cca",
      "cfe8f4111bfb405c81595bb4dcfa7935",
      "a143d39644ff4a0480bbe1649b27ef67",
      "3421b02e23c1403ba883f2e66b89de6f",
      "f5d91eef1b694de6b41ae8367028ffec",
      "063e06a480304f048dd17ee3c57fe36c",
      "4eb1207b217b457ba348f9b24667c51e",
      "2a56bb070e394e27a561009b49fd90e2",
      "681d05523b98441ba27606abb88c9546"
     ]
    },
    "id": "LZqJr5vblsEw",
    "outputId": "559d7ae8-570f-485e-9486-f5a5e6f8ccf6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Try importing flash-attention for faster inference...\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "WARNING:transformers_modules.Qwen-7B-Chat.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "107c13863fd14396bdffad51bbf0c207"
      }
     },
     "metadata": {}
    }
   ]
  }
 ]
}
